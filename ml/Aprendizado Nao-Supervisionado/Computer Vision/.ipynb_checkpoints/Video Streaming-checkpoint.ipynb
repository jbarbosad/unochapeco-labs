{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.135.1.16.2 Capacidade de carregar e analisar vídeos capturados previamente ou em tempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de vídeo e processamento em tempo real utilizando técnicas de processamento de imagem com OpenCV e Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Instalação de bibliotecas e dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in /Users/jorgechagas/anaconda3/lib/python3.11/site-packages (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0 (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.0\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0 torchvision==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/jorgechagas/anaconda3/lib/python3.11/site-packages (24.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install opencv-contrib-python==4.1.2.30\n",
    "!pip install opencv-python==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Js2Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywebrtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Setup inicial e import das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas comuns\n",
    "import numpy as np\n",
    "import argparse, io, os, json, cv2, random, imutils, glob, math, shutil, time, requests, pickle, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output, display, Javascript, Image, HTML\n",
    "\n",
    "from io import BytesIO\n",
    "from js2py import eval_js\n",
    "from base64 import b64decode\n",
    "from PIL import Image\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywebrtc import WidgetStream, VideoStream\n",
    "\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from torchvision.models import detection\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Configuração do ambiente e extração no COS (Cloud Object Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='dccd4310-ec4c-4655-9c86-318175838137', project_access_token='p-df846e5ff2cd263f175d05a3319cf08fdafb1bc9')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN = False  # USE WITH CARE! Wipe out saved files when this is true (else reuse for speed)\n",
    "input_video_url = \"https://raw.githubusercontent.com/IBM/powerai-counting-cars/master/data/test_video.mp4\"  # The input video\n",
    "START_LINE = 0  # If start line is > 0, cars won't be added until below the line (try 200)\n",
    "FRAMES_DIR = \"frames\"  # Output dir to hold/cache the original frames\n",
    "OUTPUT_DIR = \"output\"  # Output dir to hold the annotated frames\n",
    "SAMPLING = 10  # Classify every n frames (use tracking in between)\n",
    "CONFIDENCE = 0.80  # Confidence threshold to filter iffy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN:\n",
    "    if os.path.isdir(FRAMES_DIR):\n",
    "        shutil.rmtree(FRAMES_DIR)\n",
    "    if os.path.isdir(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "\n",
    "if not os.path.isdir(FRAMES_DIR):\n",
    "    os.mkdir(FRAMES_DIR)\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para lidar com arquivos compactados\n",
    "import zipfile\n",
    "def get_zip(nome_arquivo):\n",
    "    '''\n",
    "    nome_arquivo = nome do arquivo .zip que deseja baixar e extrair do Cloud Object Storage \n",
    "    '''\n",
    "    try:\n",
    "        fobj = open(nome_arquivo, \"wb\")\n",
    "        fobj.write(project.get_file(nome_arquivo).read()) \n",
    "        fobj.close()\n",
    "        z = zipfile.ZipFile(nome_arquivo)\n",
    "        z.extractall()\n",
    "    except Exception as e:\n",
    "        print(Exception,e)\n",
    "    else:\n",
    "        print('Arquivos extraídos com sucesso') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_imshow(title, image):\n",
    "  # convert the image frame BGR to RGB color space and display it\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.grid(False)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_zip(\"yolo.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_zip(\"object-detection.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Configuração da rede neural e parâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"image\": \"pytorch-object-detection/images/example_01.jpg\",\n",
    "    \"model\": \"frcnn-resnet\",\n",
    "    \"labels\": \"pytorch-object-detection/coco_classes.pickle\",\n",
    "    \"confidence\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device we will be using to run the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the list of categories in the COCO dataset and then generate a\n",
    "# set of bounding box colors for each class\n",
    "CLASSES = pickle.loads(open(args[\"labels\"], \"rb\").read())\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary containing model name and it's corresponding \n",
    "# torchvision function call\n",
    "MODELS = {\n",
    "    \"frcnn-resnet\": detection.fasterrcnn_resnet50_fpn,\n",
    "    \"frcnn-mobilenet\": detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "    \"retinanet\": detection.retinanet_resnet50_fpn\n",
    "}\n",
    "\n",
    "# load the model and set it to evaluation mode\n",
    "model = MODELS[args[\"model\"]](pretrained=True, progress=True,\n",
    "\tnum_classes=len(CLASSES), pretrained_backbone=True).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_enablement = \"\"\"</br>\n",
    "<video id=\"video\" width=\"680\" height=\"420\" autoplay></video>\n",
    "<button id=\"snap\">Processe a foto do vídeo</button>\n",
    "<canvas id=\"canvas\" width=\"680\" height=\"420\"></canvas>\n",
    "\n",
    "<script>\n",
    "// Grab elements, create settings, etc.\n",
    "var video = document.getElementById('video');\n",
    "\n",
    "// Get access to the camera!\n",
    "if(navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n",
    "    navigator.mediaDevices.getUserMedia({ video: true, audio:true }).then(function(stream) {\n",
    "        //video.src = window.URL.createObjectURL(stream);\n",
    "        //video.play();\n",
    "        video.srcObject=stream;\n",
    "        video.play();\n",
    "    });\n",
    "}\n",
    "\n",
    "// Elements for taking the snapshot\n",
    "var canvas = document.getElementById('canvas');\n",
    "var context = canvas.getContext('2d');\n",
    "var video = document.getElementById('video');\n",
    "\n",
    "// Trigger photo take\n",
    "document.getElementById(\"snap\").addEventListener(\"click\", function() {\n",
    "    context.drawImage(video, 0, 0, 680, 420);\n",
    "    var myCanvas = document.getElementById('canvas');\n",
    "    var image = myCanvas.toDataURL(\"image/png\");\n",
    "    IPython.notebook.kernel.execute(\"print('testing')\")\n",
    "    IPython.notebook.kernel.execute(\"image = '\" + image + \"'\")\n",
    "});\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_enablement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Inferência em tempo real do vídeo capturado pela câmera do usuário para detecção e reconhecimento de objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_im = Image.open(io.BytesIO(b64decode(image.split(',')[1])))\n",
    "\n",
    "# use numpy to convert the pil_image into a numpy array\n",
    "np_image = np.array(pil_im)  \n",
    "\n",
    "# convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n",
    "# the color is converted from RGB to BGR format\n",
    "imagem = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR) \n",
    "\n",
    "origem = imagem.copy()\n",
    "\n",
    "# convert the image from BGR to RGB channel ordering and change the\n",
    "# image from channels last to channels first ordering\n",
    "imagem = imagem.transpose((2, 0, 1))\n",
    "\n",
    "# add the batch dimension, scale the raw pixel intensities to the\n",
    "# range [0, 1], and convert the image to a floating point tensor\n",
    "imagem = np.expand_dims(imagem, axis=0)\n",
    "imagem = imagem / 255.0\n",
    "imagem = torch.FloatTensor(imagem)\n",
    "\n",
    "# send the input to the device and pass the it through the network to\n",
    "# get the detections and predictions\n",
    "imagem = imagem.to(DEVICE)\n",
    "detections = model(imagem)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "for i in range(0, len(detections[\"boxes\"])):\n",
    "    # extract the confidence (i.e., probability) associated with the\n",
    "    # prediction\n",
    "    confidence = detections[\"scores\"][i]\n",
    "\n",
    "    # filter out weak detections by ensuring the confidence is\n",
    "    # greater than the minimum confidence\n",
    "    if confidence > args[\"confidence\"]:\n",
    "        # extract the index of the class label from the detections,\n",
    "        # then compute the (x, y)-coordinates of the bounding box\n",
    "        # for the object\n",
    "        idx = int(detections[\"labels\"][i])\n",
    "        box = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "        # display the prediction to our terminal\n",
    "        label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "        print(\"[INFO] {}\".format(label))\n",
    "\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(origem, (startX, startY), (endX, endY),\n",
    "            COLORS[idx], 2)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        cv2.putText(origem, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "# show the output image\n",
    "plt_imshow(\"Output\", origem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_recorder = ImageRecorder(stream=camera)\n",
    "image_recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Exemplo de execução em tempo real de vídeo streaming pelo Watson Studio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"eFiNz4ZlWA0\", width=500)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Exemplo de processamento em batch de vídeo streaming por chamada URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"yolo\": \"yolo-object-detection/yolo-coco\",\n",
    "    \"confidence\": 0.5,\n",
    "    \"threshold\": 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([args[\"yolo\"], \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "    dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([args[\"yolo\"], \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([args[\"yolo\"], \"yolov3.cfg\"])\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -rtsp_transport tcp -i rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov -f image2 -vf fps=fps=10/120 img%03d.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(filename):\n",
    "    \n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    coordinates_changes = []\n",
    "    results = []\n",
    "    \n",
    "    # load our input image and grab its spatial dimensions\n",
    "    image = cv2.imread(filename)\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (960, 540),\n",
    "        swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > args[\"confidence\"]:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "    # boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, args[\"confidence\"],\n",
    "                            args[\"threshold\"])\n",
    "    \n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            # draw a bounding box rectangle and label on the image\n",
    "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "            dataset = {\"confidence\": confidences[i], \"ymax\": y+h, \"label\": LABELS[classIDs[i]], \"xmax\": x+w, \"xmin\": x, \"ymin\": y, \"attr\": []}\n",
    "            \n",
    "            results.append(dataset)\n",
    "             \n",
    "            \n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5, color, 2)\n",
    "   \n",
    "                                 \n",
    "    result = json.dumps({\"classified\": results, \"result\": \"success\"}) \n",
    "                       \n",
    "    plt_imshow(\"Image\", image)\n",
    "            \n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rc = detect_objects('img009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"jsonresp: %s\" % rc)\n",
    "if 'classified' in rc:\n",
    "    print(\"Got back %d objects\" % len(rc['classified']))\n",
    "print(json.dumps(rc, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
